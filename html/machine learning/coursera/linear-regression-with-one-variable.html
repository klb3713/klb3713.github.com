
<!DOCTYPE html><html>
<head>
    <meta http-equiv="Content-Type"
          content="text/html; charset=utf-8"/>
    
        <title>单变量线性回归 &mdash; klb3713&#39;s notebook</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link href='http://fonts.googleapis.com/css?family=Crete+Round' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" href="../../_static/sphinx-bootstrap.css" type="text/css"/>
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css"/>
    <!-- Le HTML5 shim, for IE6-8 support of HTML5 elements -->
    <!--[if lt IE 9]>
      <script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
        <script type="text/javascript">
            var DOCUMENTATION_OPTIONS = {
                URL_ROOT:'../../',
                VERSION:'1.0',
                COLLAPSE_INDEX:false,
                FILE_SUFFIX:'.html',
                HAS_SOURCE:  true
            };
        </script>
            <script type="text/javascript" src="../../_static/jquery.js"></script>
            <script type="text/javascript" src="../../_static/underscore.js"></script>
            <script type="text/javascript" src="../../_static/doctools.js"></script>
            <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
            <script type="text/javascript" src="../../_static/bootstrap-dropdown.js"></script>
            <link rel="author" title="About these documents"
                  href="../../about.html"/>
        <link rel="top" title="klb3713&#39;s notebook" href="../../index.html"/>
            <link rel="up" title="机器学习" href="../index.html"/>
            <link rel="next" title="Introduction" href="../class/introduction.html"/>
            <link rel="prev" title="引言" href="introduction.html"/> 
    <script type="text/javascript" src="https://apis.google.com/js/plusone.js"></script>
    
        <!-- analytics -->
        <script type="text/javascript">
            var _gaq = _gaq || [];
            _gaq.push(['_setAccount', 'UA-41454165-1']);
            
            _gaq.push(['_trackPageview']);

            (function () {
                var ga = document.createElement('script');
                ga.type = 'text/javascript';
                ga.async = true;
                ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
                var s = document.getElementsByTagName('script')[0];
                s.parentNode.insertBefore(ga, s);
            })();
        </script>
    
</head>
<body>

    <div class="navbar navbar-static-top">
        <div class="navbar-inner">
            <div class="container">

    <div class="related">
        <ul class="nav">
                <li class=""><a class="brand" href="../../index.html">notebook</a></li>
                <li>
                    <a href="../../genindex.html"
                       title="General Index" accesskey="I">index</a>
                </li>
                <li>
                    <a href="../class/introduction.html"
                       title="Introduction" accesskey="N">next</a>
                </li>
                <li>
                    <a href="introduction.html"
                       title="引言" accesskey="P">previous</a>
                </li>
                <li><a href="../index.html"
                        accesskey="U">机器学习</a>
                </li> 
        </ul>
    </div>
                <div class="pull-right">
                    
        <form class="navbar-search" action="../../search.html"
              method="get">
            <input type="text" name="q" placeholder="search"/>
            <input type="hidden" name="check_keywords" value="yes"/>
            <input type="hidden" name="area" value="default"/>
        </form>
                </div>
            </div>
        </div>
    </div>


<ul class="quick-links">
    <li><strong>Quick links</strong></li>
    <li>
        <a href="https://github.com/klb3713/notebook/">GitHub</a>
    </li>
    <li>
        <a href="https://weibo.com/klb3713">Weibo</a>
    </li>
    <li>
        <a href="http://www.douban.com/people/klb-3713/">Douban</a>
    </li>
    <li class="divider">&middot;</li>
    <li>
        <iframe class="github-btn"
                src="http://ghbtns.com/github-btn.html?user=klb3713&repo=notebook&type=watch&count=true"
                allowtransparency="true" frameborder="0" scrolling="0"
                width="110px" height="20px"></iframe>
    </li>
    <li>
        <iframe class="github-btn"
                src="http://ghbtns.com/github-btn.html?user=klb3713&repo=notebook&type=fork&count=true"
                allowtransparency="true" frameborder="0" scrolling="0"
                width="94px" height="20px"></iframe>
    </li>

    <li class="divider">&middot;</li>
    <li class="plusone-container">
        <g:plusone size="medium"></g:plusone>
    </li>
</ul>  
    <div class="container">
        <div class="content row">
                <div class="span9">
                    
  <div class="section" id="id1">
<h1>单变量线性回归<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h1>
<p>斯坦福大学机器学习第二课&#8221;单变量线性回归(Linear regression with one variable)“
学习笔记，本次课程主要包括7部分：</p>
<ol class="arabic simple">
<li>Model representation(模型表示</li>
<li>Cost function(代价函数，成本函数)</li>
<li>Cost function intuition I(直观解释1)</li>
<li>Cost function intuition II(直观解释2)</li>
<li>Gradient descent(梯度下降)</li>
<li>Gradient descent intuition(梯度下降直观解释)</li>
<li>Gradient descent for linear regression(应用于线性回归的的梯度下降算法)</li>
</ol>
<p>以下是第二课“单变量线性回归”的课件资料下载链接，视频可以在Coursera机器学习课程上观看或下载：
<a class="reference external" href="https://d19vezwu8eufl6.cloudfront.net/ml/docs%2Fslides%2FLecture2.pptx">PPT</a>
<a class="reference external" href="https://d19vezwu8eufl6.cloudfront.net/ml/docs%2Fslides%2FLecture2.pdf">PDF</a></p>
<div class="section" id="model-representation">
<h2>Model representation(模型表示)<a class="headerlink" href="#model-representation" title="Permalink to this headline">¶</a></h2>
<p>回到 <a class="reference internal" href="introduction.html"><em>引言</em></a> 中的房屋价格预测问题，首先它是一个有监督学习的问题
（对于每个样本的输入，都有正确的输出或者答案），同时它也是一个回归问题
（预测一个实值输出）。训练集表示如下：</p>
<a class="reference internal image-reference" href="../../_images/2-1.png"><img alt="../../_images/2-1.png" src="../../_images/2-1.png" style="width: 421.4px; height: 206.5px;" /></a>
<p>其中：</p>
<p>m = 训练样本的数目</p>
<p>x&#8217;s = “输入”变量，也称之为特征</p>
<p>y&#8217;s = “输出”变量，也称之为“目标”变量</p>
<p>对于房价预测问题，学习过程可用下图表示：</p>
<a class="reference internal image-reference" href="../../_images/2-2.png"><img alt="../../_images/2-2.png" src="../../_images/2-2.png" style="width: 333.9px; height: 237.3px;" /></a>
<p>其中x代表房屋的大小，y代表预测的价格，h(hypothesis)将输入变量 x 映射到输出变量 y，如何表示h?</p>
<p>事实上Hypothesis可以表示成如下形式：</p>
<div class="math">
\[h_\theta(x) = \theta_0 + \theta_1 x\]</div>
<p>简写为 h(x)，也就是带一个变量的线性回归或者单变量线性回归问题。</p>
</div>
<div class="section" id="cost-function">
<h2>Cost function(代价函数，成本函数)<a class="headerlink" href="#cost-function" title="Permalink to this headline">¶</a></h2>
<p>对于Hypothesis: <span class="math">\(h_\theta(x) = \theta_0 + \theta_1 x\)</span>,
<span class="math">\(\theta_i\)</span> 为参数, 如何求 <span class="math">\(\theta_i\)</span> ?</p>
<a class="reference internal image-reference" href="../../_images/2-3.png"><img alt="../../_images/2-3.png" src="../../_images/2-3.png" style="width: 290.5px; height: 209.3px;" /></a>
<p>构想： 对于训练集(x, y)，选取参数 <span class="math">\(\theta_0, \theta_1\)</span> 使得 <span class="math">\(h_\theta(x)\)</span> 尽可能的接近y。</p>
<p>如何做呢？一种做法就是求训练集的平方误差函数（squared error function），Cost Function可表示为:</p>
<div class="math">
\[J(\theta_0, \theta_1) = \frac{1}{2m}\sum_{i=1}^m{(h_\theta(x^{(i)}) - y^{(i)})^2}\]</div>
<p>并且选取合适的参数使其最小化，数学表示如下：</p>
<div class="math">
\[\displaystyle\mathop{\mathrm{minimize}}\limits_{\theta_0, \theta_1} J(\theta_0, \theta_1)\]</div>
</div>
<div class="section" id="cost-function-intuition-i-1">
<h2>Cost function intuition I(直观解释1)<a class="headerlink" href="#cost-function-intuition-i-1" title="Permalink to this headline">¶</a></h2>
<p>直观来看，线性回归主要包括如下四大部分，分别是Hypothesis, Parameters, Cost Function, Goal:</p>
<a class="reference internal image-reference" href="../../_images/2-4.png"><img alt="../../_images/2-4.png" src="../../_images/2-4.png" style="width: 334.6px; height: 308.0px;" /></a>
<p>这里作者给出了一个简化版的Cost function解释，也就是令 <span class="math">\(\theta_0\)</span> 为0：</p>
<a class="reference internal image-reference" href="../../_images/2-5.png"><img alt="../../_images/2-5.png" src="../../_images/2-5.png" style="width: 284.9px; height: 332.5px;" /></a>
<p>然后令 <span class="math">\(\theta_1\)</span> 分别取1、0.5、-0.5等值，同步对比 <span class="math">\(h_\theta(x)\)</span>
和 <span class="math">\(J(\theta_0, \theta_1)\)</span> 在二维坐标系中的变化情况，具体可参考原PPT中的对比图，很直观。</p>
</div>
<div class="section" id="cost-function-intuition-ii-2">
<h2>Cost function intuition II(直观解释2)<a class="headerlink" href="#cost-function-intuition-ii-2" title="Permalink to this headline">¶</a></h2>
<p>回顾线性回归的四个部分，这一次不再对Cost Function做简化处理，这个时候
<span class="math">\(J(\theta_0, \theta_1)\)</span> 的图形是一个三维图或者一个等高线图，具体可参考原课件。</p>
<p>可以发现，当 <span class="math">\(h_\theta(x)\)</span> 的直线越来越接近样本点时，
<span class="math">\(J(\theta_0, \theta_1)\)</span> 在等高线的图中的点越来越接近最小值的位置。</p>
</div>
<div class="section" id="gradient-descent">
<h2>Gradient descent(梯度下降)<a class="headerlink" href="#gradient-descent" title="Permalink to this headline">¶</a></h2>
<p>应用的场景之一——最小值问题：</p>
<p>对于一些函数，例如 <span class="math">\(J(\theta_0, \theta_1)\)</span>,
目标: <span class="math">\(\displaystyle\mathop{\mathrm{minimize}}\limits_{\theta_0, \theta_1} J(\theta_0, \theta_1)\)</span></p>
<p><strong>方法的框架:</strong></p>
<ol class="arabic simple">
<li>给 <span class="math">\(\theta_0, \theta_1\)</span> 一个初始值，例如都等于0</li>
<li>每次改变 <span class="math">\(\theta_0, \theta_1\)</span> 的时候都保持 <span class="math">\(J(\theta_0, \theta_1)\)</span> 递减，
直到达到一个我们满意的最小值；</li>
</ol>
<p>对于任一 <span class="math">\(J(\theta_0, \theta_1)\)</span>, 初始位置不同，最终达到的极小值点也不同，例如以下两个例子：</p>
<a class="reference internal image-reference" href="../../_images/2-6.png"><img alt="../../_images/2-6.png" src="../../_images/2-6.png" style="width: 529.2px; height: 288.4px;" /></a>
<a class="reference internal image-reference" href="../../_images/2-7.png"><img alt="../../_images/2-7.png" src="../../_images/2-7.png" style="width: 522.9px; height: 272.3px;" /></a>
<p><strong>梯度下降算法：</strong></p>
<p>重复下面的公式直到收敛：</p>
<a class="reference internal image-reference" href="../../_images/2-8.png"><img alt="../../_images/2-8.png" src="../../_images/2-8.png" style="width: 529.9px; height: 157.5px;" /></a>
<p><strong>举例：</strong></p>
<p>参数正确的更新过程如下（同步更新）：</p>
<a class="reference internal image-reference" href="../../_images/2-9.png"><img alt="../../_images/2-9.png" src="../../_images/2-9.png" style="width: 301.0px; height: 146.3px;" /></a>
<p>错误的更新过程如下：</p>
<a class="reference internal image-reference" href="../../_images/2-10.png"><img alt="../../_images/2-10.png" src="../../_images/2-10.png" style="width: 288.4px; height: 141.4px;" /></a>
</div>
<div class="section" id="gradient-descent-intuition">
<h2>Gradient descent intuition(梯度下降直观解释)<a class="headerlink" href="#gradient-descent-intuition" title="Permalink to this headline">¶</a></h2>
<p>举例，对于一个简化的 <span class="math">\(J(\theta_1)\)</span> 来说，无论抛物线的左边还是右边，
在梯度下降算法下， <span class="math">\(\theta_1\)</span> 都是保持正确的方向（递增或递减）</p>
<p>对于learning rate(又称为步长)来说:</p>
<a class="reference internal image-reference" href="../../_images/2-11.png"><img alt="../../_images/2-11.png" src="../../_images/2-11.png" style="width: 216.3px; height: 58.8px;" /></a>
<p>如果 <span class="math">\(\alpha\)</span> 过小，梯度下降可能很慢；如果过大，梯度下降有可能“迈过”（overshoot）最小点，
并且有可能收敛失败，并且产生“分歧”(diverge)</p>
<p>梯度下降可以使函数收敛到一个局部最小值，特别对于learning rate <span class="math">\(\alpha\)</span> 是固定值的时候：</p>
<a class="reference internal image-reference" href="../../_images/2-12.png"><img alt="../../_images/2-12.png" src="../../_images/2-12.png" style="width: 505.4px; height: 262.5px;" /></a>
<p>当函数接近局部最小值的时候，梯度下降法将自动的采取“小步子”， 所以没有必要随着时间的推移减小learning rate.</p>
<p>关于梯度下降算法，可以参考 <a class="reference external" href="http://zh.wikipedia.org/wiki/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95">维基百科</a> 的介绍</p>
</div>
<div class="section" id="gradient-descent-for-linear-regression">
<h2>Gradient descent for linear regression(应用于线性回归的的梯度下降算法)<a class="headerlink" href="#gradient-descent-for-linear-regression" title="Permalink to this headline">¶</a></h2>
<p>梯度下降算法：</p>
<a class="reference internal image-reference" href="../../_images/2-13.png"><img alt="../../_images/2-13.png" src="../../_images/2-13.png" style="width: 282.1px; height: 219.1px;" /></a>
<p>线性回归模型：</p>
<a class="reference internal image-reference" href="../../_images/2-14.png"><img alt="../../_images/2-14.png" src="../../_images/2-14.png" style="width: 320.6px; height: 175.7px;" /></a>
<p><span class="math">\(J(\theta_0, \theta_1)\)</span> 对 <span class="math">\(\theta_0, \theta_1\)</span> 求导得：</p>
<a class="reference internal image-reference" href="../../_images/2-15.png"><img alt="../../_images/2-15.png" src="../../_images/2-15.png" style="width: 540.4px; height: 128.8px;" /></a>
<p>在梯度下降算法中进行替换，就得到单变量线性回归梯度下降算法：</p>
<a class="reference internal image-reference" href="../../_images/2-16.png"><img alt="../../_images/2-16.png" src="../../_images/2-16.png" style="width: 607.6px; height: 265.3px;" /></a>
<p>详细的图形举例请参考官方PPT，主要是在等高线图举例梯度下降的收敛过程，
逐步逼近最小值点，其中一幅图说明：线性回归函数是凸函数(convex function)，具有碗状（bowl shape)。</p>
<p><strong>总结</strong>： 这里的梯度下降算法也称为&#8221;Batch&#8221; 梯度下降: 梯度下降的每一步都使用了所有的训练样本。</p>
</div>
</div>


                    
                        <div id="disqus_thread"></div>
                        <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
                    
                </div>
                <div class="span3">
                    
        <div class="sphinxsidebar">
            <div class="sphinxsidebarwrapper">
  <h3><a href="../../index.html">Table Of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">单变量线性回归</a><ul>
<li><a class="reference internal" href="#model-representation">Model representation(模型表示)</a></li>
<li><a class="reference internal" href="#cost-function">Cost function(代价函数，成本函数)</a></li>
<li><a class="reference internal" href="#cost-function-intuition-i-1">Cost function intuition I(直观解释1)</a></li>
<li><a class="reference internal" href="#cost-function-intuition-ii-2">Cost function intuition II(直观解释2)</a></li>
<li><a class="reference internal" href="#gradient-descent">Gradient descent(梯度下降)</a></li>
<li><a class="reference internal" href="#gradient-descent-intuition">Gradient descent intuition(梯度下降直观解释)</a></li>
<li><a class="reference internal" href="#gradient-descent-for-linear-regression">Gradient descent for linear regression(应用于线性回归的的梯度下降算法)</a></li>
</ul>
</li>
</ul>

  <h4>Previous topic</h4>
  <p class="topless"><a href="introduction.html"
                        title="previous chapter">引言</a></p>
  <h4>Next topic</h4>
  <p class="topless"><a href="../class/introduction.html"
                        title="next chapter">Introduction</a></p>
  <h3>This Page</h3>
  <ul class="this-page-menu">
    <li><a href="../../_sources/machine learning/coursera/linear-regression-with-one-variable.txt"
           rel="nofollow">Show Source</a></li>
  </ul>
            </div>
        </div>
                </div>
            
        </div>
    </div>
    <div class="footer">
        
        <div class="container">
                    &copy;
                        Copyright 2013, klb3713.
                Created using <a
                        href="http://sphinx.pocoo.org/">Sphinx</a> 1.1.3
                    .
                Theme <a href="https://github.com/scotch/sphinx-bootstrap">Sphinx-Bootstrap</a>
                adapted from
                <a href="http://twitter.github.com/bootstrap/">Twitter's
                    Bootstrap</a> by <a href="http://www.scotchmedia.com">Scotch
                Media</a>
        </div>
    </div>


    <!-- disqus -->
    <script type="text/javascript">
        var disqus_title = '单变量线性回归 &mdash; klb3713&#39;s notebook';
        var disqus_identifier ='code' + '.' + "klb3713" + window.location.pathname.replace('/', '.');
    </script>

    <script type="text/javascript">
        /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
        var disqus_shortname = "klb3713"; // required: replace example with your forum shortname

        /* * * DON'T EDIT BELOW THIS LINE * * */
        (function() {
            var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
            dsq.src = 'http://' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();
    </script>


<script type="text/javascript">
    $('.nav .active').on('click', function () {
        e.preventDefault()
        $(this).siblings().toggle()
    });
</script>

<!-- plusone -->
<script type="text/javascript">
    (function () {
        var po = document.createElement('script');
        po.type = 'text/javascript';
        po.async = true;
        po.src = 'https://apis.google.com/js/plusone.js';
        var s = document.getElementsByTagName('script')[0];
        s.parentNode.insertBefore(po, s);
    })();
</script>

<!-- twitter -->
<script src="http://platform.twitter.com/widgets.js" type="text/javascript"></script>

</body>
</html>